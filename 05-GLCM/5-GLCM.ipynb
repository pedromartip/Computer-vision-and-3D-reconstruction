{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e6bbf0-2773-4a95-986a-49d90043ff18",
   "metadata": {},
   "source": [
    "# GLCM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a4cf40-4c9d-49f1-9edd-068731ee0d0a",
   "metadata": {},
   "source": [
    "Algorithm theory: https://prism.ucalgary.ca/bitstream/handle/1880/51900/texture%20tutorial%20v%203_0%20180206.pdf?sequence=11&isAllowed=y\n",
    "\n",
    "Next link contains an usage case of the library: https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_glcm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72b320e6-60c2-41a2-8220-9e5d204a8ce6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'greycomatrix' from 'skimage.feature' (c:\\Users\\ca01770\\Anaconda3\\envs\\AIV_UIB\\Lib\\site-packages\\skimage\\feature\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ca01770\\Documents\\GitHub\\Computer-vision-and-3D-reconstruction\\05-GLCM\\5-GLCM.ipynb Celda 3\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ca01770/Documents/GitHub/Computer-vision-and-3D-reconstruction/05-GLCM/5-GLCM.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ca01770/Documents/GitHub/Computer-vision-and-3D-reconstruction/05-GLCM/5-GLCM.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ca01770/Documents/GitHub/Computer-vision-and-3D-reconstruction/05-GLCM/5-GLCM.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mskimage\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature\u001b[39;00m \u001b[39mimport\u001b[39;00m greycomatrix, greycoprops\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ca01770/Documents/GitHub/Computer-vision-and-3D-reconstruction/05-GLCM/5-GLCM.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mskimage\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'greycomatrix' from 'skimage.feature' (c:\\Users\\ca01770\\Anaconda3\\envs\\AIV_UIB\\Lib\\site-packages\\skimage\\feature\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "from skimage import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2281be82-ac62-4941-a861-fa1305868fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sample image\n",
    "image = data.camera()\n",
    "plt.imshow(image, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c75d3c-90cc-4406-a8c2-600ca16fd278",
   "metadata": {},
   "source": [
    "First, we select a set of patches from the image. We select 4 grass patches and 4 sky patches, and we register them.\n",
    "Next code select those patches manually and save them into two lists.\n",
    "\n",
    "Remember, that a patch is an image piece. In our case we have defined it as a square of $21 \\times 21$ pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f90fdb6-00ff-4b0b-a076-d73b0e302f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2611e0-5449-463b-a7b6-edf8165f2c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select some patches from grassy areas of the image\n",
    "grass_locations = [(280, 454), (342, 223), (444, 192), (455, 455)]\n",
    "grass_patches = []\n",
    "for loc in grass_locations:\n",
    "    grass_patches.append(image[loc[0]:loc[0] + PATCH_SIZE,\n",
    "                               loc[1]:loc[1] + PATCH_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37c57db-7b32-49a3-94a3-9c1e6e126915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select some patches from sky areas of the image\n",
    "sky_locations = [(38, 34), (139, 28), (37, 437), (145, 379)]\n",
    "sky_patches = []\n",
    "for loc in sky_locations:\n",
    "    sky_patches.append(image[loc[0]:loc[0] + PATCH_SIZE,\n",
    "                             loc[1]:loc[1] + PATCH_SIZE])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd71d4-ce4d-447b-a039-4adaaf081aed",
   "metadata": {},
   "source": [
    "Once we have the patches selected, we can compute its GLCM features.\n",
    "The computation procedure of this kind of features consists in two steps:\n",
    "1. Compute the Grey Level Co-occurrence Matrix (la GLCM).\n",
    "2. Compute the texture properties of the GLCM.\n",
    "\n",
    "The first step is achieved applying the function [greycomatrix](https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.graycomatrix) and the second one using [greycoprops](https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.graycoprops)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4963eb7-36c9-43e8-b5d5-1c6f203be00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute some GLCM properties each patch\n",
    "xs = []\n",
    "ys = []\n",
    "for patch in (grass_patches + sky_patches):\n",
    "    glcm = greycomatrix(patch, \n",
    "                        distances=[1],\n",
    "                        angles=[0], # We ignore this\n",
    "                        levels=256, # Number of possible pixel values\n",
    "                        symmetric=True, \n",
    "                        normed=True)\n",
    "    xs.append(greycoprops(glcm, 'dissimilarity')[0, 0])\n",
    "    ys.append(greycoprops(glcm, 'correlation')[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618424c0-011a-4ba2-b8c3-f3bc2986d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "glcm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42e6a1e-5dde-4bcd-8b3a-9007152c9da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124c5a08-2c3c-4348-9f02-9294a2913e3e",
   "metadata": {},
   "source": [
    "Next, we can visualise the selected patches and the computed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c827df-0292-40d0-a8cd-7a552864d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the figure\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "# display original image with locations of patches\n",
    "ax = fig.add_subplot(3, 2, 1)\n",
    "ax.imshow(image, cmap=plt.cm.gray, vmin=0, vmax=255)\n",
    "for (y, x) in grass_locations:\n",
    "    ax.plot(x + PATCH_SIZE / 2, y + PATCH_SIZE / 2, 'gs')\n",
    "\n",
    "for (y, x) in sky_locations:\n",
    "    ax.plot(x + PATCH_SIZE / 2, y + PATCH_SIZE / 2, 'bs')\n",
    "\n",
    "ax.set_xlabel('Original Image')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.axis('image')\n",
    "\n",
    "# for each patch, plot (dissimilarity, correlation)\n",
    "ax = fig.add_subplot(3, 2, 2)\n",
    "ax.plot(xs[:len(grass_patches)], ys[:len(grass_patches)], 'go', label='Grass')\n",
    "ax.plot(xs[len(grass_patches):], ys[len(grass_patches):], 'bo', label='Sky')\n",
    "ax.set_xlabel('GLCM Dissimilarity')\n",
    "ax.set_ylabel('GLCM Correlation')\n",
    "ax.legend()\n",
    "\n",
    "# display the image patches\n",
    "for i, patch in enumerate(grass_patches):\n",
    "    ax = fig.add_subplot(3, len(grass_patches), len(grass_patches)*1 + i + 1)\n",
    "    ax.imshow(patch, cmap=plt.cm.gray, vmin=0, vmax=255)\n",
    "    ax.set_xlabel('Grass %d' % (i + 1))\n",
    "\n",
    "for i, patch in enumerate(sky_patches):\n",
    "    ax = fig.add_subplot(3, len(sky_patches), len(sky_patches)*2 + i + 1)\n",
    "    ax.imshow(patch, cmap=plt.cm.gray, vmin=0, vmax=255)\n",
    "    ax.set_xlabel('Sky %d' % (i + 1))\n",
    "\n",
    "# display the patches and plot\n",
    "fig.suptitle('Grey level co-occurrence matrix features', fontsize=14, y=1.05)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0836202-5b51-43eb-a489-a7776df3a7fb",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2827a776-ed47-40b5-98e7-ad93416485cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fm.utils.mnist_reader as mnist_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3207e492-d46e-4c57-ad3b-9b60fc66ad48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/zalandoresearch/fashion-mnist.git fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0dda1f-5706-4468-b704-97f8c9ef3221",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_images, y_train = mnist_reader.load_mnist('fm/data/fashion', kind='train')\n",
    "X_test_images, y_test = mnist_reader.load_mnist('fm/data/fashion', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03acc4f-72d9-4307-a4b7-d4f3f85bd367",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d4c16-195e-4c2f-a03e-8bb7fe56441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = X_train_images[0].reshape((28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1dd302-6249-4ed4-8171-40edc371c133",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52318d8b-bbd2-4d3f-920c-55ca3ff5e931",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Tasks\n",
    "\n",
    "Build your own classifier and extract metrics to know how the classifier is working.\n",
    "\n",
    "Detect with which classes is the classifier working better and worse. Try to explain the reasons.\n",
    "\n",
    "> **HINT:** The confusion matrix can be very helpful.\n",
    "\n",
    "> Use an image subset to speed up the procedure.\n",
    "\n",
    "Useful link: https://towardsdatascience.com/multi-class-classification-extracting-performance-metrics-from-the-confusion-matrix-b379b427a872\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f5e6b3-7cad-415d-bc32-981088124a87",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Features library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db815a15-919c-4f13-94a7-69c540e88ecb",
   "metadata": {},
   "source": [
    "https://github.com/explainingAI/uib_vfeatures\n",
    "\n",
    "\n",
    "https://pypi.org/project/uib-vfeatures/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60c4154-ef1b-46d7-80f5-1a2d24621e1c",
   "metadata": {},
   "source": [
    "# Tasks Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bba27f1-8f7e-42a2-b0ae-00c04dadcebc",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Choose Train and Validation images\n",
    "2. Define patches and features\n",
    "3. Build Train and Validation matrices\n",
    "4. Train a KNN classifier algorithm\n",
    "5. Evaluate the classifier\n",
    "6. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4ad813-8b59-48f2-b784-9e8e9be1a8f5",
   "metadata": {},
   "source": [
    "## 1. Choose Train and Validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abac9ac-3912-4e0a-a696-2fa94032a557",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_images = ...\n",
    "y_train = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d108f2c-b0fb-436a-a919-d40d9e85a6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_images = ...\n",
    "y_val = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef1bd21-3cb3-4297-b2df-06b54d179f90",
   "metadata": {},
   "source": [
    "## 2. Define patches and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688ee626-9a6a-4bf7-9848-26f707bbe7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1876c0-2217-4a22-a140-166041657bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_patches(image):\n",
    "    patches = []\n",
    "    for y in range(0, image.shape[0] - PATCH_SIZE + 1, PATCH_SIZE):\n",
    "        for x in range(0, image.shape[1] - PATCH_SIZE + 1, PATCH_SIZE):\n",
    "            patches.append(image[y:y + PATCH_SIZE, x:x + PATCH_SIZE])\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faad141c-96a5-4bab-abb6-356ed45e5207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and draw the patches of an image\n",
    "\n",
    "# patches = get_image_patches(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f59f39-fdc9-475d-b293-8d0eb3a26d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the features of the image patches as example\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "for patch in patches:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2171c685-1517-45a2-8b03-0b0830258b16",
   "metadata": {},
   "source": [
    "### Features description\n",
    "\n",
    "TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9166861-2e33-41bf-a523-eb8a289fee8e",
   "metadata": {},
   "source": [
    "## 3. Build Train and Validation matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04e902b-be4b-4cbd-9f17-17687976f1ac",
   "metadata": {},
   "source": [
    "Train and Validations matrices must have a shape of num_images x num_features.\n",
    "\n",
    "Each row in the matrices will represent an image and each column will represent an specific feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ce37e-a59f-4a1d-bab7-edf24f82e341",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = ...\n",
    "X_val   = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14373509-b0f0-4a55-97f9-1f3a298bef35",
   "metadata": {},
   "source": [
    "## 4. Train a KNN classifier algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0c407d-440c-4ced-b872-ffff48afa14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3bfbd1-053f-466e-aff1-f5b3f3c2dfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd83276-1dfd-4117-95ef-fbf33818f405",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = knn.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aba5f5-e442-42bd-b602-3d781aca4b50",
   "metadata": {},
   "source": [
    "## 5. Evaluate the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c986286-e120-48fe-ba9f-035fd1a82359",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"T-shirt\", \"trousers\", \"pull-over\" ,\"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80e185a-ca1c-4a3f-8b48-dafa69b77696",
   "metadata": {},
   "source": [
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_estimator\n",
    "\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41c7ae9-852b-4e7a-96e0-91cb0a83e70b",
   "metadata": {},
   "source": [
    "## 6. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15acf33f-ba54-411a-8edc-727860eb7e42",
   "metadata": {},
   "source": [
    "Take your own conclusions of the results and the execution of the procedure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
